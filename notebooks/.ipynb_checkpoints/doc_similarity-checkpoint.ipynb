{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the document pairwise similarity between the different input data sets and print the most common document terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def clean_text(raw_text, remove_stopwords = False, output_format =\"string\"):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            raw_text: raw text from input\n",
    "            remove_stopwords: a boolean variable to indicate whether to remove stop words\n",
    "            output_format: if \"string\", return a cleaned string\n",
    "                           if \"list\", a list of words extracted from cleaned string.\n",
    "    Output:\n",
    "            Cleaned string or list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove HTML markup\n",
    "    text = BeautifulSoup(raw_text, \"lxml\")\n",
    "\n",
    "    # Keep only characters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text.get_text())\n",
    "\n",
    "    # Split words and store to list\n",
    "    text = text.lower().split()\n",
    "\n",
    "    if remove_stopwords:\n",
    "\n",
    "        # Use set as it has O(1) lookup time\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in text if w not in stops]\n",
    "\n",
    "    else:\n",
    "        words = text\n",
    "\n",
    "    # Return a cleaned string or list\n",
    "    if output_format == \"string\":\n",
    "        return \" \".join(words)\n",
    "\n",
    "    elif output_format == \"list\":\n",
    "        return words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.757040959382\n",
      "  (0, 2)\t0.842257819759\n",
      "  (0, 3)\t0.685120659518\n",
      "  (0, 0)\t1.0\n",
      "  (1, 0)\t0.757040959382\n",
      "  (1, 2)\t0.854170354095\n",
      "  (1, 3)\t0.821569594602\n",
      "  (1, 1)\t1.0\n",
      "  (2, 0)\t0.842257819759\n",
      "  (2, 1)\t0.854170354095\n",
      "  (2, 3)\t0.78159738371\n",
      "  (2, 2)\t1.0\n",
      "  (3, 0)\t0.685120659518\n",
      "  (3, 1)\t0.821569594602\n",
      "  (3, 2)\t0.78159738371\n",
      "  (3, 3)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_files = ['ucb_comments.csv',\n",
    "              '../training_data/hope_tweets_test.txt',\n",
    "              '../training_data/depressed_tweets_test.txt',\n",
    "              '../training_data/fact_tweets_test.txt']\n",
    "\n",
    "documents = [open(f).read() for f in text_files]\n",
    "documents = [clean_text(d) for d in documents]\n",
    "tfidf = TfidfVectorizer().fit_transform(documents)\n",
    "# no need to normalize, since Vectorizer will return normalized tf-idf\n",
    "pairwise_similarity = tfidf * tfidf.T\n",
    "print pairwise_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "most common document terms/occurrences for  ucb_comments.csv\n",
      "epilepsy 868\n",
      "seizures 865\n",
      "seizure 627\n",
      "one 468\n",
      "get 444\n",
      "years 425\n",
      "know 370\n",
      "like 352\n",
      "sleep 348\n",
      "since 315\n",
      "time 312\n",
      "people 311\n",
      "would 283\n",
      "life 253\n",
      "never 249\n",
      "day 243\n",
      "go 216\n",
      "help 215\n",
      "always 214\n",
      "son 209\n",
      "still 209\n",
      "also 203\n",
      "work 203\n",
      "old 185\n",
      "even 183\n",
      "meds 183\n",
      "take 179\n",
      "going 179\n",
      "brain 176\n",
      "right 171\n",
      "mal 170\n",
      "good 170\n",
      "partial 166\n",
      "free 164\n",
      "memory 163\n",
      "feel 162\n",
      "daughter 161\n",
      "drive 159\n",
      "grand 158\n",
      "remember 156\n",
      "back 154\n",
      "need 153\n",
      "many 152\n",
      "family 151\n",
      "could 150\n",
      "thank 149\n",
      "year 147\n",
      "much 146\n",
      "really 145\n",
      "god 143\n",
      "\n",
      "most common document terms/occurrences for  ../training_data/hope_tweets_test.txt\n",
      "day 58\n",
      "hope 51\n",
      "god 30\n",
      "people 28\n",
      "tomorrow 26\n",
      "try 25\n",
      "today 23\n",
      "end 23\n",
      "good 23\n",
      "life 23\n",
      "one 22\n",
      "give 21\n",
      "sometimes 21\n",
      "lord 20\n",
      "voice 20\n",
      "going 19\n",
      "never 18\n",
      "always 16\n",
      "us 16\n",
      "even 15\n",
      "new 15\n",
      "love 15\n",
      "must 14\n",
      "saying 14\n",
      "get 14\n",
      "courage 14\n",
      "see 14\n",
      "quiet 14\n",
      "mary 14\n",
      "things 14\n",
      "like 14\n",
      "radmacher 14\n",
      "anne 14\n",
      "live 13\n",
      "thing 13\n",
      "better 13\n",
      "need 13\n",
      "make 13\n",
      "let 12\n",
      "turn 12\n",
      "die 11\n",
      "world 11\n",
      "everything 11\n",
      "hogshead 10\n",
      "changed 10\n",
      "sort 10\n",
      "care 10\n",
      "little 10\n",
      "nothing 10\n",
      "sally 10\n",
      "\n",
      "most common document terms/occurrences for  ../training_data/depressed_tweets_test.txt\n",
      "people 36\n",
      "feel 35\n",
      "day 33\n",
      "like 33\n",
      "may 29\n",
      "get 26\n",
      "life 23\n",
      "mood 22\n",
      "got 21\n",
      "one 20\n",
      "really 20\n",
      "need 20\n",
      "know 20\n",
      "feeling 18\n",
      "sad 18\n",
      "today 17\n",
      "go 17\n",
      "someone 15\n",
      "want 14\n",
      "love 14\n",
      "going 14\n",
      "see 14\n",
      "inside 13\n",
      "time 13\n",
      "think 13\n",
      "good 13\n",
      "much 12\n",
      "talk 12\n",
      "still 12\n",
      "everyone 12\n",
      "wish 12\n",
      "would 11\n",
      "work 11\n",
      "na 11\n",
      "many 11\n",
      "symptoms 11\n",
      "sometimes 11\n",
      "prince 11\n",
      "back 11\n",
      "friends 11\n",
      "hate 10\n",
      "better 10\n",
      "even 10\n",
      "u 10\n",
      "moment 10\n",
      "could 10\n",
      "im 9\n",
      "months 9\n",
      "year 9\n",
      "always 9\n",
      "\n",
      "most common document terms/occurrences for  ../training_data/fact_tweets_test.txt\n",
      "people 22\n",
      "first 18\n",
      "best 14\n",
      "one 13\n",
      "ever 12\n",
      "di 12\n",
      "ia 12\n",
      "dan 12\n",
      "never 11\n",
      "u 11\n",
      "life 10\n",
      "world 10\n",
      "made 10\n",
      "yang 9\n",
      "dia 9\n",
      "water 9\n",
      "good 9\n",
      "time 9\n",
      "know 9\n",
      "like 9\n",
      "god 8\n",
      "even 8\n",
      "de 8\n",
      "el 8\n",
      "say 8\n",
      "two 8\n",
      "adalah 8\n",
      "person 8\n",
      "go 8\n",
      "que 8\n",
      "every 7\n",
      "work 7\n",
      "much 7\n",
      "na 7\n",
      "years 7\n",
      "average 7\n",
      "baekhyun 7\n",
      "dengan 7\n",
      "day 7\n",
      "karena 6\n",
      "human 6\n",
      "give 6\n",
      "se 6\n",
      "million 6\n",
      "thing 6\n",
      "think 6\n",
      "por 6\n",
      "th 6\n",
      "company 6\n",
      "someone 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx,doc in enumerate(documents):\n",
    "    wordlist = nltk.FreqDist(nltk.word_tokenize(clean_text(doc, True)))\n",
    "    print \"\\nmost common document terms/occurrences for \", text_files[idx]\n",
    "    for item in wordlist.most_common(50):\n",
    "        print item[0], item[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
